{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jadson/anaconda3/envs/py/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import shutil\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectar (ou criar) um banco de dados\n",
    "con = duckdb.connect(database='jus.duckdb', read_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "CREATE TABLE metadadosPublicacao202202 AS SELECT * FROM read_json_auto('./data/202202-stj/metadadosPublicacao202202.json');\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabela de texto dos juiz\n",
    "con.execute(\"\"\"\n",
    "  CREATE TABLE decisions_description (\n",
    "    ID INT PRIMARY KEY,\n",
    "    content TEXT,\n",
    "    summarize TEXT\n",
    "  );\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argostranslate.package\n",
    "import argostranslate.translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the benefits of banana?\n"
     ]
    }
   ],
   "source": [
    "# Download and install Argos Translate package\n",
    "argostranslate.package.update_package_index()\n",
    "available_packages = argostranslate.package.get_available_packages()\n",
    "package_to_install = next(\n",
    "    filter(\n",
    "        lambda x: x.from_code == \"pt\" and x.to_code == \"en\", available_packages\n",
    "    )\n",
    ")\n",
    "argostranslate.package.install_from_path(package_to_install.download())\n",
    "\n",
    "# Translate\n",
    "translatedText = argostranslate.translate.translate(\"Quais são os benefícios de banana ?\", \"pt\", \"en\")\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments(text, tokenizer):\n",
    "    # Split text into sentences\n",
    "    sentences = text.splitlines()\n",
    "\n",
    "    segments = []\n",
    "    current_segment = np.array([], dtype=int)\n",
    "    current_token_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True).input_ids[0].numpy()\n",
    "        token_count = tokens.shape[0]\n",
    "\n",
    "        # If the sentence itself is larger than 512 tokens, we need to handle it separately\n",
    "        if token_count >= 512:\n",
    "            # Save current segment if not empty\n",
    "            if current_segment.size > 0:\n",
    "                segments.append(current_segment)\n",
    "                current_segment = np.array([], dtype=int)\n",
    "                current_token_count = 0\n",
    "\n",
    "            # Split the large sentence and add to segments\n",
    "            for i in range(0, token_count, 512):\n",
    "                segments.append(tokens[i:i+512])\n",
    "            continue\n",
    "\n",
    "        # Add tokens to current segment or create new segment\n",
    "        if current_token_count + token_count < 512:\n",
    "            current_segment = np.concatenate((current_segment, tokens))\n",
    "            current_token_count += token_count\n",
    "        else:\n",
    "            segments.append(current_segment)\n",
    "            current_segment = tokens\n",
    "            current_token_count = token_count\n",
    "\n",
    "    if current_segment.size:\n",
    "        segments.append(current_segment)\n",
    "\n",
    "    result = []\n",
    "    # Preenchendo segmentos com zeros para que tenham tamanho 512\n",
    "    for segment in segments:\n",
    "        if segment.size > 0:\n",
    "            padding_length = 512 - segment.shape[0]\n",
    "            result.append(np.concatenate([segment, np.zeros(padding_length, dtype=int)]))\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = create_segments(\"Quais são os benefícios de banana ?\",tokenizer_trad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment in segments:\n",
    "    translated = m_trad.generate(torch.tensor([segment]))\n",
    "    print(translated)\n",
    "    for t in translated:\n",
    "        print(tokenizer_trad.decode(t, skip_special_tokens=True) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/202202-stj/20220201/94552609.txt\", 'r') as f:\n",
    "  content = f.read()\n",
    "  translatedText = argostranslate.translate.translate(content, \"pt\", \"en\")\n",
    "  print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_encoded_text(encoded_text, max_length=512):\n",
    "    total_length = encoded_text['input_ids'].shape[1]\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, total_length, max_length):\n",
    "        end = min(i+max_length, total_length)\n",
    "        chunk = {\n",
    "            \"input_ids\": encoded_text[\"input_ids\"][0, i:end].unsqueeze(0),\n",
    "            \"attention_mask\": encoded_text[\"attention_mask\"][0, i:end].unsqueeze(0)\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "DELETE FROM metadadosPublicacao202202 WHERE teor NOT IN ('Concedendo', 'Negando') or teor IS NULL;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concedendo_label = 1\n",
    "negando_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = con.execute(\"SELECT seqDocumento FROM metadadosPublicacao202202\").fetchnumpy()\n",
    "data[\"seqDocumento\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"seqDocumento\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho da pasta contendo os arquivos de texto\n",
    "folder_path = './data/202202-stj/'\n",
    "index = 0\n",
    "# Percorre a pasta e suas subpastas de forma recursiva\n",
    "for subdir, dirs, files in os.walk(folder_path):\n",
    "    for filename in files:\n",
    "        index = index + 1\n",
    "        if index > 4226:\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(subdir, filename)\n",
    "                # Extrai o ID do nome do arquivo\n",
    "                file_id = int(filename.split('.')[0])\n",
    "                if np.where(data[\"seqDocumento\"] == file_id)[0].size == 1:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        content = f.read()\n",
    "                        translatedText = argostranslate.translate.translate(content, \"pt\", \"en\")\n",
    "                        try:\n",
    "                            con.execute('INSERT INTO decisions_description (id,content) VALUES (?,?)', (file_id, translatedText))\n",
    "                        except Exception as e:\n",
    "                            if 'violates primary key constraint' in str(e):\n",
    "                                print(\"Duplicate key, ignoring.\")\n",
    "                            else:\n",
    "                                raise\n",
    "                        print(index, file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values and reset the index\n",
    "data = data.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teor_type = LabelEncoder()\n",
    "data['teor_encoder'] = teor_type.fit_transform(data['teor'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para saber a correspondência entre o valor codificado e o rótulo original:\n",
    "for index, label in enumerate(teor_type.classes_):\n",
    "    print(f\"{index}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para saber a correspondência entre o valor codificado e o rótulo original:\n",
    "for index, label in enumerate(teor_type.classes_):\n",
    "    print(f\"{index}: {label}\")\n",
    "    con.execute(\"\"\"\n",
    "    UPDATE metadadosPublicacao202202\n",
    "    SET teor = ?\n",
    "    WHERE teor = ?;\n",
    "    \"\"\",(index,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = con.execute(\"SELECT seqDocumento,teor FROM metadadosPublicacao202202\").df()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
